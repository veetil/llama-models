import os
import json
import sys
import time
import gc
from dataclasses import dataclass
from pathlib import Path
from typing import Generator, List, Optional, Tuple

import torch
import torch.nn.functional as F
from fairscale.nn.model_parallel.initialize import (
    get_model_parallel_rank,
    initialize_model_parallel,
    model_parallel_is_initialized,
)
from termcolor import cprint

from ...datatypes import RawContent, RawMessage, StopReason, ToolPromptFormat
from ..api.args import ModelArgs
from ..api.chat_format import ChatFormat, LLMInput
from ..api.tokenizer import Tokenizer
from .model import Transformer

# -----------------------------------------------------------------------------
# Environment Setup
# -----------------------------------------------------------------------------
# Configure CUDA allocator to allow expandable segments (helps mitigate fragmentation).
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# -----------------------------------------------------------------------------
# Data Classes for Prediction Results
# -----------------------------------------------------------------------------
@dataclass
class CompletionPrediction:
    """
    Represents the result of a text completion.
    
    Attributes:
        generation: The generated text.
        decoded_tokens: (Optional) A list of decoded tokens.
        logprobs: (Optional) A list of log probability lists per token.
    """
    generation: str
    decoded_tokens: Optional[List[str]] = None
    logprobs: Optional[List[List[float]]] = None


@dataclass
class ChatPrediction:
    """
    Represents the result of a chat completion.
    
    Attributes:
        generation: The raw message generated.
        decoded_tokens: (Optional) A list of decoded tokens.
        logprobs: (Optional) A list of log probability lists per token.
    """
    generation: RawMessage
    decoded_tokens: Optional[List[str]] = None
    logprobs: Optional[List[List[float]]] = None


@dataclass
class TokenResult:
    """
    Represents the result of processing a single token during generation.
    
    Attributes:
        token: The token id.
        text: The decoded token text.
        logprobs: (Optional) Log probabilities associated with this token.
    """
    token: int
    text: str
    logprobs: Optional[List[float]] = None


# -----------------------------------------------------------------------------
# Helper Functions
# -----------------------------------------------------------------------------
def find_answer_span(
    candidate_ids: List[int],
    tokenizer: Tokenizer,
    answer_start_pattern: str = "answer>",
    answer_end_pattern: str = "</answer"
) -> Optional[Tuple[int, int]]:
    """
    Finds the span of the answer within a list of token ids by searching for 
    the provided start and end patterns.
    
    Args:
        candidate_ids: List of token ids generated by the model.
        tokenizer: Tokenizer instance for encoding and decoding.
        answer_start_pattern: Pattern marking the start of the answer (default "answer>").
        answer_end_pattern: Pattern marking the end of the answer (default "</answer").
    
    Returns:
        A tuple (start_index, end_index) if the answer span is found, otherwise None.
    """
    answer_start_ids = tokenizer.encode(answer_start_pattern, bos=False, eos=False)
    answer_end_ids = tokenizer.encode(answer_end_pattern, bos=False, eos=False)
    
    n = len(candidate_ids)
    start_idx = -1
    end_idx = -1

    for i in range(n - len(answer_start_ids) + 1):
        if candidate_ids[i : i + len(answer_start_ids)] == answer_start_ids:
            start_idx = i + len(answer_start_ids)
            break
    if start_idx == -1:
        return None

    for j in range(start_idx, n - len(answer_end_ids) + 1):
        if candidate_ids[j : j + len(answer_end_ids)] == answer_end_ids:
            end_idx = j
            break
    if end_idx == -1 or end_idx <= start_idx:
        return None

    return start_idx, end_idx


def compute_average_margin(
    logits_list: List[torch.Tensor], answer_ids: List[int]
) -> Tuple[List[float], float]:
    """
    Computes the margin (difference between the top two probabilities) for each token 
    in the answer span and returns both the list of margins and their average.
    
    Args:
        logits_list: List of logits tensors corresponding to generated tokens.
        answer_ids: List of token ids for the answer span.
    
    Returns:
        A tuple containing:
          - A list of margins for each token.
          - The average margin.
    """
    margins = []
    for logit, _ in zip(logits_list, answer_ids):
        logit = logit.detach()
        probs = torch.softmax(logit, dim=-1)
        if probs.size(-1) > 1:
            top_vals, _ = torch.topk(probs, k=2)
            top_vals = top_vals.squeeze(0)
            token_margin = (top_vals[0] - top_vals[1]).item()
        else:
            token_margin = 1.0
        margins.append(token_margin)
    avg_margin = sum(margins) / len(margins) if margins else 0.0
    return margins, avg_margin


# -----------------------------------------------------------------------------
# Llama Model Class Definition
# -----------------------------------------------------------------------------
class Llama:
    @staticmethod
    def build(
        ckpt_dir: str,
        max_seq_len: int,
        max_batch_size: int,
        model_parallel_size: Optional[int] = None,
        tokenizer_path: Optional[str] = None,
        seed: int = 1,
        device: str = "cuda"
    ):
        """
        Initializes the Llama model by loading checkpoints, setting up the 
        tokenizer, and initializing model parallelism.
        
        Args:
            ckpt_dir: Directory containing checkpoint files.
            max_seq_len: Maximum sequence length for the model.
            max_batch_size: Maximum batch size for inference.
            model_parallel_size: (Optional) Size for model parallelism.
            tokenizer_path: (Optional) Path to the tokenizer.
            seed: Random seed for reproducibility.
            device: Device type ('cuda', 'cpu', or 'xpu').
        
        Returns:
            An instance of Llama with the model, tokenizer, and model arguments.
        """
        device = torch.device(device)
        if (device.type == "cuda" and not torch.cuda.is_available()) or \
           (device.type == "xpu" and not torch.xpu.is_available()):
            raise RuntimeError(f"PyTorch backend for {device.type} device type is not available")

        if not torch.distributed.is_initialized():
            torch.distributed.init_process_group("nccl" if device.type == "cuda" else "gloo")

        if not model_parallel_is_initialized():
            if model_parallel_size is None:
                model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
            initialize_model_parallel(model_parallel_size)

        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        if device.type == "cuda":
            torch.cuda.set_device(local_rank)
        elif device.type == "xpu":
            torch.xpu.set_device(local_rank)

        torch.manual_seed(seed)
        if local_rank > 0:
            sys.stdout = open(os.devnull, "w")

        start_time = time.time()
        checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
        assert len(checkpoints) > 0, f"no checkpoint files found in {ckpt_dir}"
        assert model_parallel_size == len(checkpoints), (
            f"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}"
        )
        ckpt_path = checkpoints[get_model_parallel_rank()]
        checkpoint = torch.load(ckpt_path, map_location="cpu", weights_only=True)
        with open(Path(ckpt_dir) / "params.json", "r") as f:
            params = json.loads(f.read())

        model_args: ModelArgs = ModelArgs(
            max_seq_len=max_seq_len,
            max_batch_size=max_batch_size,
            **params,
        )
        tokenizer = Tokenizer(model_path=tokenizer_path) if tokenizer_path else Tokenizer.get_instance()
        assert model_args.vocab_size == tokenizer.n_words

        torch.set_default_device(device)
        if device.type == "cuda":
            torch.set_default_dtype(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.half)
        elif device.type == "xpu":
            torch.set_default_dtype(torch.bfloat16 if torch.xpu.is_bf16_supported() else torch.half)
        else:
            torch.set_default_dtype(torch.half)

        if model_args.vision_chunk_size > 0:
            from .multimodal.model import CrossAttentionTransformer
            model = CrossAttentionTransformer(model_args)
            model.setup_cache(model_args.max_batch_size, torch.get_default_dtype())
        else:
            model = Transformer(model_args)
        model.load_state_dict(checkpoint, strict=True)
        model.to(device)
        print(f"Loaded in {time.time() - start_time:.2f} seconds")
        return Llama(model, tokenizer, model_args)

    def __init__(self, model: Transformer, tokenizer: Tokenizer, args: ModelArgs):
        """
        Initializes the Llama instance with a model, tokenizer, and configuration.
        """
        self.args = args
        self.model = model
        self.tokenizer = tokenizer
        self.formatter = ChatFormat(tokenizer)

    @torch.inference_mode()
    def generate(
        self,
        model_input: LLMInput,
        max_gen_len: int,
        temperature: float = 0.6,
        top_p: float = 0.9,
        logprobs: bool = False,
        echo: bool = False,
        print_model_input: bool = False,
        cot_decoding: bool = False,
        cot_top_k: int = 10,
        answer_start_pattern: str = "answer>",
        answer_end_pattern: str = "</answer"
    ) -> Generator:
        """
        Generates tokens based on the provided input using either standard greedy
        decoding or Chain-of-Thought (CoT) decoding.
        
        Args:
            model_input: The input prompt wrapped in an LLMInput object.
            max_gen_len: Maximum tokens to generate.
            temperature: Sampling temperature.
            top_p: Nucleus sampling parameter.
            logprobs: Whether to compute token log probabilities.
            echo: Whether to include input tokens in the output.
            print_model_input: Optionally print the decoded model input.
            cot_decoding: Whether to enable Chain-of-Thought decoding.
            cot_top_k: Number of alternatives for branching in CoT decoding.
            answer_start_pattern: Pattern marking the beginning of the answer.
            answer_end_pattern: Pattern marking the end of the answer.
        
        Yields:
            TokenResult objects for each generated token.
        """
        params = self.model.params

        if print_model_input:
            tokens_to_print = [
                self.formatter.vision_token if t == 128256 else t
                for t in model_input.tokens
            ]
            cprint("Input to model:\n" + self.tokenizer.decode(tokens_to_print) + "\n", "red")
        prompt_tokens = [model_input.tokens]
        bsz = 1
        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)

        min_prompt_len = min(len(t) for t in prompt_tokens)
        max_prompt_len = max(len(t) for t in prompt_tokens)
        if max_prompt_len >= params.max_seq_len:
            cprint(f"Out of token budget {max_prompt_len} vs {params.max_seq_len}", "red")
            return

        total_len = min(max_gen_len + max_prompt_len, params.max_seq_len)
        is_vision = not isinstance(self.model, Transformer)
        if is_vision:
            images = model_input.vision.images if model_input.vision is not None else []
            mask = model_input.vision.mask if model_input.vision is not None else []
            xattn_caches, cross_attention_masks, full_text_row_masked_out_mask = \
                self.model.compute_vision_tokens_masks(
                    batch_images=[images],
                    batch_masks=[mask],
                    total_len=total_len,
                )

        pad_id = self.tokenizer.pad_id
        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long)
        for k, t in enumerate(prompt_tokens):
            tokens[k, :len(t)] = torch.tensor(t, dtype=torch.long)
        if logprobs:
            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)

        prev_pos = 0
        eos_reached = torch.tensor([False] * bsz)
        input_text_mask = tokens != pad_id

        if echo:
            for i, t in enumerate(model_input.tokens):
                yield TokenResult(
                    token=t,
                    text=self.tokenizer.decode([t]),
                    logprobs=(token_logprobs[0, i:i+1].tolist() if logprobs else None),
                )

        stop_tokens = torch.tensor(self.tokenizer.stop_tokens)

        # --- Chain-of-Thought (CoT) Decoding Branch ---
        if str(cot_decoding).lower() == "true":
            cur_pos = min_prompt_len
            if is_vision:
                position_ids = torch.arange(prev_pos, cur_pos, dtype=torch.long)
                logits = self.model.forward(
                    position_ids,
                    tokens,
                    cross_attention_masks,
                    full_text_row_masked_out_mask,
                    xattn_caches,
                    model_input.vision is None,
                )
            else:
                logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
            first_token_logits = logits[:, -1, :]
            _, top_k_indices = torch.topk(first_token_logits, cot_top_k)
            candidate_results = []
            
            for idx in top_k_indices[0]:
                candidate_tokens = tokens.clone()
                candidate_tokens[0, cur_pos] = idx

                candidate_generated_logits = []
                for pos in range(cur_pos + 1, total_len):
                    if is_vision:
                        position_ids = torch.arange(prev_pos, pos, dtype=torch.long)
                        step_logits = self.model.forward(
                            position_ids,
                            candidate_tokens,
                            cross_attention_masks,
                            full_text_row_masked_out_mask,
                            xattn_caches,
                            model_input.vision is None,
                        )
                    else:
                        step_logits = self.model.forward(candidate_tokens[:, prev_pos:pos], prev_pos)
                    step_logits = step_logits.detach()
                    step_probs = torch.softmax(step_logits[:, -1] / temperature, dim=-1)
                    next_tok = torch.argmax(step_probs, dim=-1).reshape(-1)
                    candidate_tokens[0, pos] = next_tok
                    candidate_generated_logits.append(step_logits[:, -1, :].cpu())
                    if torch.isin(next_tok, stop_tokens).all():
                        break

                cand_ids = candidate_tokens[0].tolist()
                span = find_answer_span(
                    cand_ids[min_prompt_len:],
                    self.tokenizer,
                    answer_start_pattern,
                    answer_end_pattern,
                )
                if span is None:
                    del candidate_generated_logits
                    torch.cuda.empty_cache()
                    continue
                start_idx, end_idx = span
                if start_idx < 0 or end_idx > len(candidate_generated_logits):
                    del candidate_generated_logits
                    torch.cuda.empty_cache()
                    continue
                logits_slice = candidate_generated_logits[start_idx:end_idx]
                answer_token_ids = cand_ids[min_prompt_len + start_idx:min_prompt_len + end_idx]
                margins_list, avg_margin = compute_average_margin(logits_slice, answer_token_ids)
                candidate_results.append((candidate_tokens[0], avg_margin))
                del candidate_generated_logits, candidate_tokens
                torch.cuda.empty_cache()
                gc.collect()

            if candidate_results:
                best_candidate, _ = max(candidate_results, key=lambda x: x[1])
                for pos in range(min_prompt_len, len(best_candidate)):
                    yield TokenResult(
                        token=best_candidate[pos],
                        text=self.tokenizer.decode([best_candidate[pos]])
                    )
                return

        # --- Standard Greedy Decoding ---
        for cur_pos in range(min_prompt_len, total_len):
            if is_vision:
                position_ids = torch.arange(prev_pos, cur_pos, dtype=torch.long)
                logits = self.model.forward(
                    position_ids,
                    tokens,
                    cross_attention_masks,
                    full_text_row_masked_out_mask,
                    xattn_caches,
                    model_input.vision is None,
                )
            else:
                logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)
            if temperature > 0:
                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)
                next_token = sample_top_p(probs, top_p)
            else:
                next_token = torch.argmax(logits[:, -1], dim=-1)
            next_token = next_token.reshape(-1)
            next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)
            tokens[:, cur_pos] = next_token

            target = tokens[:, prev_pos + 1:cur_pos + 1]
            if is_vision:
                vision_tokens = [self.tokenizer.special_tokens["<|image|>"], 128256]
                masks = [target.eq(t) for t in vision_tokens]
                mask = torch.logical_or(*masks) if len(masks) > 1 else masks[0]
                target[mask] = 0
            if logprobs:
                token_logprobs[:, prev_pos + 1:cur_pos + 1] = -F.cross_entropy(
                    input=logits.transpose(1, 2),
                    target=target,
                    reduction="none",
                    ignore_index=pad_id,
                )
            eos_reached |= (~input_text_mask[:, cur_pos]) & (torch.isin(next_token, stop_tokens))
            yield TokenResult(
                token=next_token[0].item(),
                text=self.tokenizer.decode(next_token.tolist()),
                logprobs=(token_logprobs[:, cur_pos:cur_pos + 1][0].tolist() if logprobs else None)
            )
            prev_pos = cur_pos
            if all(eos_reached):
                break

    def text_completion(
        self,
        content: RawContent,
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        logprobs: bool = False,
        echo: bool = False,
        cot_decoding: bool = False,
        cot_top_k: int = 10,
    ) -> CompletionPrediction:
        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:
            max_gen_len = self.model.params.max_seq_len - 1

        model_input = self.formatter.encode_content(content)
        tokens = []
        token_logprobs = []
        decoded_tokens = []
        for result in self.generate(
            model_input=model_input,
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=logprobs,
            echo=echo,
            cot_decoding=cot_decoding,
            cot_top_k=cot_top_k,
        ):
            tokens.append(result.token)
            if logprobs:
                decoded_tokens.append(result.text)
                token_logprobs.append(result.logprobs)
        generation = self.tokenizer.decode(tokens)
        if logprobs:
            return CompletionPrediction(
                generation=generation,
                logprobs=token_logprobs,
                decoded_tokens=decoded_tokens,
            )
        return CompletionPrediction(generation=generation)

    def chat_completion(
        self,
        messages: List[RawMessage],
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        logprobs: bool = False,
        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,
        echo: bool = False,
        cot_decoding: bool = False,
        cot_top_k: int = 10,
    ) -> ChatPrediction:
        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:
            max_gen_len = self.model.params.max_seq_len - 1

        tokens = []
        token_logprobs = []
        decoded_tokens = []
        stop_reason = None
        for result in self.generate(
            model_input=self.formatter.encode_dialog_prompt(messages, tool_prompt_format),
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=logprobs,
            echo=echo,
            cot_decoding=cot_decoding,
            cot_top_k=cot_top_k,
        ):
            tokens.append(result.token)
            if result.text == "<|eot_id|>":
                stop_reason = StopReason.end_of_turn
            elif result.text == "<|eom_id|>":
                stop_reason = StopReason.end_of_message
            if logprobs:
                decoded_tokens.append(result.text)
                token_logprobs.append(result.logprobs)
        if stop_reason is None:
            stop_reason = StopReason.out_of_tokens
        message = self.formatter.decode_assistant_message(tokens, stop_reason)
        if logprobs:
            return ChatPrediction(
                generation=message,
                logprobs=token_logprobs,
                decoded_tokens=decoded_tokens,
            )
        return ChatPrediction(generation=message)

    def chat_completion_raw(
        self,
        messages: List[RawMessage],
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
        tool_prompt_format: ToolPromptFormat = ToolPromptFormat.json,
    ) -> List[int]:
        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:
            max_gen_len = self.model.params.max_seq_len - 1
        output_tokens = []
        model_input = self.formatter.encode_dialog_prompt(messages, tool_prompt_format)
        input_tokens = model_input.tokens
        for result in self.generate(
            model_input=model_input,
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=False,
        ):
            output_tokens.append(result.token)
        return input_tokens, output_tokens

    def text_completion_raw(
        self,
        content: RawContent,
        temperature: float = 0.6,
        top_p: float = 0.9,
        max_gen_len: Optional[int] = None,
    ):
        if max_gen_len is None or max_gen_len == 0 or max_gen_len >= self.model.params.max_seq_len:
            max_gen_len = self.model.params.max_seq_len - 1
        model_input = self.formatter.encode_content(content)
        input_tokens = model_input.tokens
        output_tokens = []
        for result in self.generate(
            model_input=model_input,
            max_gen_len=max_gen_len,
            temperature=temperature,
            top_p=top_p,
            logprobs=False
        ):
            output_tokens.append(result.token)
        return input_tokens, output_tokens


def sample_top_p(probs, p):
    """
    Samples the next token based on top-p (nucleus) sampling.
    
    Args:
        probs (torch.Tensor): Probability distribution tensor.
        p (float): Cumulative probability threshold.
    
    Returns:
        torch.Tensor: Sampled token id.
    """
    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
    probs_sum = torch.cumsum(probs_sort, dim=-1)
    mask = probs_sum - probs_sort > p
    probs_sort[mask] = 0.0
    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
    next_token = torch.multinomial(probs_sort, num_samples=1)
    next_token = torch.gather(probs_idx, -1, next_token)
    return next_token
